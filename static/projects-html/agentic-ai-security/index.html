<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Agentic AI Security: Trustworthy AI for Cybersecurity</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description" content="Project page for: Agentic AI Security. Trustworthy AI for cybersecurity, focusing on networked and communications-enabled systems, agentic AI, and generative models." />
  <link href="https://fonts.googleapis.com/css2?family=Google+Sans:wght@400;600;700&family=Noto+Sans:wght@400;600&display=swap" rel="stylesheet">
  <style>
    :root{
      --bg:#ffffff; --text:#1f2937; --muted:#6b7280; --soft:#f8fafc; --line:#e5e7eb; --card:#ffffff;
      --btn:#111827; --btnText:#ffffff; --link:#2563eb;
      --wrap:1100px;
    }
    @media (prefers-color-scheme: dark){
      :root{
        --bg:#0b0e12; --text:#e5e7eb; --muted:#9aa4b2; --soft:#0f1319; --line:#1f2733; --card:#0d1117;
        --btn:#1f2937; --btnText:#e5e7eb; --link:#8ab4ff;
      }
    }
    *{box-sizing:border-box}
    html,body{margin:0;background:var(--bg);color:var(--text);font-family:"Noto Sans",system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif}
    a{color:var(--link);text-decoration:none}
    a:hover{text-decoration:underline}
    .container{max-width:var(--wrap);margin:0 auto;padding:0 20px}
    header.hero{padding:56px 0 22px}
    h1{font-family:"Google Sans",sans-serif;font-weight:700;line-height:1.12;letter-spacing:.2px;
       font-size:clamp(28px,5.2vw,56px);text-align:center;margin:0 0 12px}
    .venue{font-family:"Google Sans",sans-serif;text-align:center;color:var(--muted);font-size:20px;margin:4px 0 10px}
    .authors,.affils{font-size:16px;text-align:center}
    .authors a{color:#2a6bf4}
    .authors sup{font-size:12px;vertical-align:top}
    .affils{color:var(--muted);line-height:1.6;margin-top:6px}
    .buttons{display:flex;gap:14px;justify-content:center;flex-wrap:wrap;margin:20px 0 6px}
    .btn{display:inline-flex;align-items:center;gap:10px;padding:12px 18px;border-radius:999px;border:1px solid var(--line);
         background:var(--btn);color:var(--btnText);font-weight:700}
    .btn.light{background:transparent;color:var(--text)}
    .btn:hover{opacity:.95;text-decoration:none}
    section{padding:40px 0;border-top:1px solid var(--line)}
    h2{font-family:"Google Sans",sans-serif;font-size:30px;margin:0 0 14px;text-align:center}
    .content{max-width:900px;margin:0 auto;font-size:17px;line-height:1.7}
    .figure{max-width:980px;margin:16px auto 6px}
    .figure img{width:100%;height:auto;border-radius:12px;background:var(--soft);display:block}
    .caption{color:var(--muted);font-size:14px;text-align:center;margin-top:6px}
    .grid-2{display:grid;grid-template-columns:1fr;gap:16px}
    @media (min-width:920px){ .grid-2{grid-template-columns:1fr 1fr} }
    pre{background:var(--soft);border:1px solid var(--line);border-radius:10px;padding:14px;overflow:auto}
    footer{padding:34px 0;text-align:center;color:var(--muted);font-size:14px}
  </style>
</head>
<body>
<header class="hero container">
  <h1>Agentic AI Security</h1>
  <div class="venue">Research Project 2026</div>
  <div class="authors">
    <a href="https://scholar.google.com/citations?user=1meq3gcAAAAJ&hl=en" target="_blank" rel="noopener">Md Nazmul Kabir Sikder</a><sup>1</sup>
  </div>
  <div class="affils">
    <sup>1</sup>CoVA CCI, Old Dominion University, Norfolk, VA, USA
  </div>
  <div class="buttons">
    <a class="btn light" href="https://github.com/AI-VTRC/Agentic-AI-Security" target="_blank" rel="noopener">ðŸ’» Code (GitHub)</a>
  </div>
</header>
<section id="abstract">
  <div class="container">
    <h2>Introduction</h2>
    <div class="content">
      <p>My research develops trustworthy AI methods for cybersecurity, with emphasis on networked and communications-enabled systems and AI-enabled cyber-physical systems (CPS). Critical infrastructure increasingly relies on AI deployed over networks, from cloud-hosted AI-as-a-service to edge intelligence embedded in operational technology (OT). This shift expands the attack surface: adversaries can exploit model vulnerabilities, manipulate streaming data, poison learning pipelines, and misuse generative or agentic AI to scale attacks. My goal is to build robust, explainable, and security-aware AI that remains reliable under adversarial pressure and supports high-stakes operational decision-making.</p>
      <p>My work spans (1) CPS security and anomaly detection in water and wastewater systems, (2) AI assurance methods for measuring reliability, robustness, and data fidelity, and (3) emerging directions in generative AI security, including detecting synthetic content and using synthetic data responsibly for evaluation. I have published peer-reviewed research in these areas, contributed to NSF proposal development, released open-source artifacts, and collaborated with interdisciplinary teams to translate research into practical cyber-defense capabilities. Across these efforts, the central principle is that "trust" must be earned through explicit threat models, measurable assurance signals, and deployable workflows that remain stable as networks, sensors, and adversaries evolve. Figure 1 summarizes my agenda across three threat surfaces, including networked CPS, networked AI and agentic workflows, and generative models, and highlights the corresponding research thrusts and operational outcomes.</p>
      <div class="figure">
      <img src="/media/projects/agentic-ai-security/agentic-ai-wf.jpg" alt="Agentic AI Security research workflow diagram." />
      <p class="caption">Overview of my research program in trustworthy AI for cybersecurity: three threat surfaces (networked CPS, networked AI services/agentic workflows, and generative models) motivate three research thrusts (CPS security, AI assurance, and generative AI security), producing operationally actionable outcomes supported by a cross-cutting emphasis on threat models, reproducible evaluation, open-source artifacts, mentoring, and external funding.</p>
      </div>
    </div>
  </div>
</section>
<section id="overview">
  <div class="container">
    <h2>Research Thrusts</h2>
    <div class="content">
      <h3>1. Trustworthy AI for Security of Networked CPS and Critical Infrastructure</h3>
      <p>A major driver of my research is the need for AI defenses in networked CPS where sensing, control, and telemetry are tightly coupled and operate under uncertainty. In water and wastewater infrastructure, cyber incidents often appear as subtle changes in sensor patterns, actuator behavior, or operational sequences, and these signals are frequently masked by noise, nonstationarity, and normal operational variability. I develop learning-based detection and forecasting methods that are accurate and operationally trustworthy, with attention to reliability, low false-alarm rates, and decision support. In practice, this means building models that do more than flag anomalies, they provide operator-relevant evidence about what changed, where it changed, and why it likely matters, so responses are safe and efficient rather than driven by spurious alarms.</p>
      <p>My prior work includes deep learning approaches for cyberattack detection in water systems and AI-driven frameworks for wastewater operations and security. I have also explored context-aware forecasting, where upstream factors and external context shape system dynamics and the interpretation of anomalies. Looking forward, I will generalize these efforts to broader communications-enabled CPS and networked control environments, including industrial networks and other critical systems where telemetry may be distributed, delayed, or partially observed. A concrete focus is adversary-aware anomaly detection that separates natural disturbances (legitimate transients, weather-driven effects, maintenance activity) from coordinated manipulations across multi-sensor environments. Closely related is robust learning under distribution shift, targeting changing network conditions, sensor drift, and evolving operational regimes, because CPS security models often fail when they silently assume yesterday's statistics will hold tomorrow. Finally, I emphasize operational trust and interpretability through actionable explanations, such as identifying which networked signals, subsystems, or control loops most influenced a detection decision, and presenting these explanations in forms aligned with engineering workflows (e.g., subsystem-level attribution, event timelines, and operator-facing summaries rather than abstract feature importances).</p>
      <p><em>This thrust aligns with ODU's interest in AI-enabled CPS security and trustworthy AI integrated into networked systems, especially where communications constraints, partial observability, and operational procedures define what effective security looks like.</em></p>
      <h3>2. AI Assurance for Networked AI Systems and AI-as-a-Service Security</h3>
      <p>As AI increasingly runs "over the network", through cloud inference APIs, distributed model serving, federated learning, and agentic workflows, trustworthy AI must go beyond accuracy and address security, provenance, auditability, and risk measurement. My research develops model-agnostic assurance methods and evaluation frameworks that quantify trust under realistic constraints and threat models. The key idea is that modern deployments are not isolated models; they are networked services embedded in pipelines for data collection, preprocessing, training, serving, monitoring, and decision-making. Attacks can occur at many points, and failures can cascade across components.</p>
      <p>I focus on assurance methods that answer the questions stakeholders care about: when should we trust a model, when is it failing silently, what evidence supports a security decision, and how can we detect manipulation in data pipelines or learning updates? These questions are central to networked deployments, where adversaries can target data streams, inference traffic, identity and authentication boundaries, and update mechanisms. Within this thrust, I plan to develop assurance metrics and protocols for networked AI services, including reliability scoring under attack and drift, and confidence calibration designed for security decisions where false confidence can be more harmful than abstaining. I also focus on secure learning pipelines, including detection of poisoning and backdoors in training data and model updates, particularly in distributed or collaborative settings where provenance is incomplete and updates arrive asynchronously. In addition, I am increasingly interested in trustworthy agentic AI for cyber operations, where LLM- or tool-using agents interact with logs, alerts, and telemetry; here the goal is to design guardrails and verification layers that enforce bounded actions and durable audit trails, so agentic assistance improves analyst effectiveness without creating opaque or exploitable automation.</p>
      <p><em>This thrust directly supports the position's emphasis on AI-driven security analysis on networked systems, threat intelligence, and security of networked AI-as-a-service systems, and it complements growing momentum in LLM and agentic AI security.</em></p>
      <h3>3. Generative AI Security, Synthetic Data, and Detection/Attribution for Cyber Defense</h3>
      <p>Generative models are fundamentally dual use. They can support privacy-preserving data sharing and simulation, but they also enable deception, impersonation, and scalable adversarial content. My research addresses both sides: I develop principled methods for using synthetic data in security contexts, and I build detection and attribution methods for synthetic and adversarial content. I approach this space with two commitments. First, synthetic data must preserve scientific validity and operational relevance, not only visual or statistical plausibility. Second, defenses must generalize beyond the specific generator or attack used during evaluation, because real adversaries do not follow benchmark constraints.</p>
      <p>I have worked on the fidelity and utility of synthetic data generation for cyber-physical domains and developed datasets and benchmarking frameworks for synthetic and adversarial image detection. Going forward, I will extend these ideas to security settings that include evaluating intrusion detection models with realistic synthetic traces, detecting manipulated sensor and telemetry patterns, and understanding how generative content affects trust in decision pipelines. Within this thrust, I plan to generate realistic CPS and network traces with known ground truth to stress-test detectors under rare or dangerous scenarios that are difficult to collect in the wild. I also plan to advance detection and attribution methods that identify synthetic or manipulated content and connect it to model families or generation pipelines, supporting threat intelligence and incident response. Finally, I will study concealment and fabrication attacks in vision and time-series pipelines and develop defenses that generalize across unseen generators and perturbation strategiesâ€”an increasingly important requirement given rapid advances in diffusion models and tool-augmented generation workflows.</p>
      <p><em>This thrust complements ODU's Trustworthy AI initiative and supports interdisciplinary collaboration across cybersecurity, engineering, and data science at the intersection of provenance, reliability, and security evaluation.</em></p>
      <!-- <h3>Funding Strategy and Collaboration at ODU</h3>
      <p>My funding plan targets programs aligned with trustworthy AI and networked CPS security, including NSF SaTC, NSF CPS, and mission-driven opportunities through ONR and DARPA. At ODU, I am excited by the joint ecosystem between ECE and the School of Cybersecurity and by collaboration opportunities across the Trustworthy AI cluster. I plan to build a research group that supports high-impact publications and externally funded projects, with strong mentoring of Ph.D. students and meaningful undergraduate research involvement. Across projects, I will prioritize outputs that are scientifically rigorous and practically reusable, including open-source evaluation harnesses, reproducible datasets where permissible, and deployment-aware prototypes that shorten the path from ideas to operational impact.</p>
      <h3>Integration with Teaching and Student Mentoring</h3>
      <p>My teaching interests include Trustworthy AI, AI for cybersecurity, secure ML for CPS, and generative AI security, with an emphasis on hands-on assignments using real security data and reproducible pipelines. I mentor students to develop end-to-end research capability, from problem formulation and dataset design to rigorous evaluation, open-source release, and publication. This training directly supports ODU's goal of graduating students who can contribute to both foundational research and real-world cyber defense. In my lab culture, students learn to define threat models precisely, justify metrics and baselines, validate claims under distribution shift and adversarial settings, and communicate results in ways that are useful to both researchers and operational stakeholders.</p>
      <p><strong>In summary</strong>, my research advances trustworthy AI for networked and communications-enabled systems through (1) security-focused learning for networked CPS, (2) assurance and measurement for networked AI services and agentic workflows, and (3) generative AI security through synthetic data methodology and detection/attribution. This agenda aligns with ODU's Trustworthy AI initiative and the advertised position's emphasis on AI and network/communications security.</p> -->
    </div>
  </div>
</section>
<section id="bibtex">
  <div class="container">
    <h2>BibTeX</h2>
    <div class="content">
<pre><code>@article{Sikder2026AgenticAI,
  title   = {Agentic AI Security: Trustworthy AI for Cybersecurity},
  author  = {Sikder, Md Nazmul Kabir},
  journal = {Research Project},
  year    = {2026},
  month   = {January}
}
</code></pre>
    </div>
  </div>
</section>
<footer>
  <div class="container">
    Inspired by modern CVPR-style project pages. Â© 2026 Md Nazmul Kabir Sikder.
  </div>
</footer>
</body>
</html>
